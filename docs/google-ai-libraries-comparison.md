# Google AI Libraries: Vertex AI vs Gemini API

## Pourquoi RagForge utilise Vertex AI (`@google-cloud/vertexai`)

### BibliothÃ¨ques Google AI

Il existe **deux** faÃ§ons d'utiliser les modÃ¨les Google :

#### 1. **Gemini API** - `@google/genai`
- ğŸ”‘ **Auth:** API Key (simple)
- ğŸŒ **Endpoint:** generativelanguage.googleapis.com
- ğŸ’° **Pricing:** Pay-per-use, free tier disponible
- ğŸ¯ **Use case:** Prototypage, petits projets, dÃ©veloppement rapide

#### 2. **Vertex AI** - `@google-cloud/vertexai`
- ğŸ” **Auth:** Service Account (GCP)
- ğŸŒ **Endpoint:** {region}-aiplatform.googleapis.com
- ğŸ’° **Pricing:** GCP billing, pas de free tier
- ğŸ¯ **Use case:** Production, enterprise, scaling

### Pourquoi RagForge utilise Vertex AI ?

**Raisons actuelles:**

1. **Embeddings de production**
   - `text-embedding-004` est disponible sur Vertex AI
   - Meilleure intÃ©gration avec Neo4j (mÃªme infra GCP)
   - Quotas plus Ã©levÃ©s (1000 req/min vs 15 req/min)

2. **DÃ©jÃ  configurÃ©**
   - Service account setup pour LR_CodeRag
   - GOOGLE_APPLICATION_CREDENTIALS dÃ©jÃ  en place
   - Pas besoin d'ajouter une deuxiÃ¨me auth

3. **Production-ready**
   - SLAs garantis
   - Support entreprise
   - Monitoring avec Cloud Console

4. **Consistency**
   - MÃªme SDK pour embeddings ET gÃ©nÃ©ration
   - Une seule configuration

### Comparaison dÃ©taillÃ©e

| Feature | Gemini API | Vertex AI |
|---------|-----------|-----------|
| **Setup** | âœ… Simple (API key) | âš ï¸  Complex (service account) |
| **Auth** | API key string | JSON credentials file |
| **Free tier** | âœ… Oui (60 req/min) | âŒ Non |
| **Rate limits** | 15 req/min (paid) | 1000+ req/min |
| **Pricing** | MÃªme prix | MÃªme prix |
| **Models** | Tous Gemini + Gemma | Tous Gemini + Gemma |
| **Embeddings** | âŒ Pas text-embedding-004 | âœ… text-embedding-004 |
| **RÃ©gion** | Global | Configurable (us-central1, etc) |
| **Monitoring** | âŒ Basique | âœ… Cloud Console complet |
| **SLA** | âŒ Best effort | âœ… Garanti |

### Pour le Reranking LLM

**Option 1: Rester sur Vertex AI** â­ **RecommandÃ©**

```typescript
import { VertexAI } from '@google-cloud/vertexai';

const vertexAI = new VertexAI({
  project: process.env.VERTEX_PROJECT_ID,
  location: 'us-central1'
});

const model = vertexAI.getGenerativeModel({
  model: 'gemma-3n-e2b-it'
});

const result = await model.generateContent(prompt);
```

**Avantages:**
- âœ… CohÃ©rent avec le reste de RagForge
- âœ… MÃªmes credentials
- âœ… Meilleurs quotas (important pour parallÃ©lisation)
- âœ… Monitoring unifiÃ©

**Option 2: Ajouter Gemini API**

```typescript
import { GoogleGenAI } from '@google/genai';

const genAI = new GoogleGenAI({
  apiKey: process.env.GEMINI_API_KEY
});

const model = genAI.getGenerativeModel({
  model: 'gemma-3n-e2b-it'
});

const result = await model.generateContent(prompt);
```

**Avantages:**
- âœ… Plus simple (juste API key)
- âœ… Gratuit pour dev (60 req/min)

**InconvÃ©nients:**
- âŒ Quotas limitÃ©s (15 req/min paid)
- âŒ DeuxiÃ¨me auth Ã  configurer
- âŒ IncohÃ©rent avec embeddings

### Recommendation

**Pour RagForge:** **Rester sur Vertex AI** ğŸ¯

Raisons:
1. DÃ©jÃ  configurÃ© et fonctionnel
2. Meilleurs quotas (essentiel pour parallÃ©lisation)
3. CohÃ©rence avec embeddings
4. Production-ready

### Code unifiÃ©

CrÃ©er une abstraction LLMProvider qui fonctionne avec les deux:

```typescript
interface LLMProvider {
  generateContent(prompt: string): Promise<string>;
}

class VertexAIProvider implements LLMProvider {
  private model: any;

  constructor(config: { project: string; location: string; model: string }) {
    const vertexAI = new VertexAI({
      project: config.project,
      location: config.location
    });
    this.model = vertexAI.getGenerativeModel({ model: config.model });
  }

  async generateContent(prompt: string): Promise<string> {
    const result = await this.model.generateContent(prompt);
    return result.response.candidates[0].content.parts[0].text;
  }
}

class GeminiAPIProvider implements LLMProvider {
  private model: any;

  constructor(config: { apiKey: string; model: string }) {
    const genAI = new GoogleGenAI({ apiKey: config.apiKey });
    this.model = genAI.getGenerativeModel({ model: config.model });
  }

  async generateContent(prompt: string): Promise<string> {
    const result = await this.model.generateContent(prompt);
    return result.response.candidates[0].content.parts[0].text;
  }
}
```

Ainsi on peut facilement switch selon l'environnement:

```typescript
const provider = process.env.USE_VERTEX_AI === 'true'
  ? new VertexAIProvider({
      project: process.env.VERTEX_PROJECT_ID,
      location: 'us-central1',
      model: 'gemma-3n-e2b-it'
    })
  : new GeminiAPIProvider({
      apiKey: process.env.GEMINI_API_KEY,
      model: 'gemma-3n-e2b-it'
    });
```

### Pricing: Identique!

Les deux utilisent les **mÃªmes prix** pour les mÃªmes modÃ¨les:

- Gemma 3n E2B: $0.005 / 1M tokens (input)
- Gemini 2.5 Flash: $0.075 / 1M tokens (input)

Pas de diffÃ©rence de coÃ»t entre Gemini API et Vertex AI.

### Conclusion

**RagForge continuera d'utiliser Vertex AI** pour:
- Embeddings (text-embedding-004)
- LLM Reranking (gemma-3n-e2b-it)
- Future gÃ©nÃ©ration de code

**Pourquoi?**
- DÃ©jÃ  setup âœ…
- Meilleurs quotas âœ…
- Production-ready âœ…
- CohÃ©rence âœ…

Le seul moment oÃ¹ Gemini API aurait du sens:
- Prototypage trÃ¨s rapide sans GCP
- Free tier pour tests

Mais pour RagForge (outil de production), **Vertex AI est le bon choix**. ğŸ¯
